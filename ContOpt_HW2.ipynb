{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Optimisation HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import scipy\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "data = loadmat('data.mat')\n",
    "data_toy = loadmat('data-toy.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Implement phi(x, P), bigphi(X, P), f(X, P, y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "# Vectorise all the functions\n",
    "\n",
    "def h(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Gaussian filter\n",
    "\n",
    "    :x: np.ndarray[(1, 2)]\n",
    "    :returns: float\n",
    "    \"\"\"\n",
    "    global sigmasquared\n",
    "    return np.e**(-np.inner(x, x)/sigmasquared)  # Always take sigma = 0.1\n",
    "\n",
    "\n",
    "def phi(x,P):\n",
    "    \"\"\"\n",
    "    Calculate contribution of each 'true' star to observed image\n",
    "    \n",
    "    :x: np.ndarray[(2, 1)]\n",
    "    :P: np.ndarray[(2, n**2)]\n",
    "    :returns: np.ndarray[(n**2, 1)]\n",
    "    \"\"\"\n",
    "    global sigmasquared\n",
    "    return (np.e**((-1/sigmasquared)*np.sum((P.T-x.T)**2,axis=1))).reshape((-1, 1), order=\"F\")\n",
    "\n",
    "\n",
    "def bigphi(X, P):\n",
    "    \"\"\"\n",
    "    Calculate image observed, based on K-star positions X\n",
    "    \n",
    "    :X: np.ndarray[(2, K)]\n",
    "    :P: np.ndarray[(2, n**2)]\n",
    "    :returns: np.ndarray[(n**2, 1)]\n",
    "    \"\"\"\n",
    "    global K, n\n",
    "    X = X.flatten(order=\"F\")\n",
    "    #non-vectorised code just in case we need it\n",
    "    bigphi = np.zeros((n**2, 1))\n",
    "    for i in range(K):\n",
    "        bigphi += phi(X[2*i:2*i+2], P)\n",
    "    #return phi(X, P)\n",
    "    return bigphi\n",
    "\n",
    "\n",
    "def  f(X, P, y):\n",
    "    \"\"\"\n",
    "    Calculate squared error of estimate bigphi(X)\n",
    "    \n",
    "    :X: np.ndarray[(2, K)]\n",
    "    :P: np.ndarray[(2, n**2)]\n",
    "    :y: np.ndarray[(n**2, 1)]\n",
    "    :returns: float\n",
    "    \"\"\"\n",
    "    global n\n",
    "    return (1/(2*n**2)) * np.linalg.norm(bigphi(X, P)-y)**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that $f$ is not convex. There are clear local maxima which cannot occur if $f$ were convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1\n",
    "n = 2\n",
    "sigma = float(data['sigma'])\n",
    "sigmasquared = sigma**2\n",
    "true_positions = np.array([[0], [0]])\n",
    "positions = np.array([[0.5, 0.5], [-0.5, 0.5], [-0.5, -0.5], [0.5, -0.5]]).T\n",
    "y = np.array([[0], [0], [1], [0]])\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "x_vals = np.linspace(-1, 1, 100)\n",
    "y_vals = np.linspace(-1, 1, 100)\n",
    "x_vals, y_vals = np.meshgrid(x_vals, y_vals)\n",
    "grid = np.array([x_vals, y_vals]).reshape((2, 100**2), order=\"F\")\n",
    "z_vals = np.zeros((1, 10000))\n",
    "for i in range(10000):\n",
    "    z_vals[0, i] = f(grid[:, i].reshape((2, 1), order=\"F\"), positions, y)\n",
    "z_vals = z_vals.reshape((100, 100), order=\"F\")\n",
    "ax.scatter(x_vals, y_vals, z_vals)\n",
    "\n",
    "plt.title('Plot of f(x) to test convexity', fontsize=24)\n",
    "plt.show()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "$ \\phi(x) = $ $\\begin{bmatrix}\n",
    "h(p_1 - x) \\\\\n",
    "\\vdots \\\\\n",
    "h(p_{n^2} - x)\n",
    "\\end{bmatrix} $\n",
    "$=$ $\\begin{bmatrix}\n",
    "e^{-\\frac{\\|p_1 - x\\|^2}{\\sigma^2}} \\\\\n",
    "\\vdots \\\\\n",
    "e^{-\\frac{\\|p_{n^2} - x\\|^2}{\\sigma^2}}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "$D\\phi(x) = $ $\\begin{bmatrix}\n",
    "\\frac{\\partial \\phi_1}{\\partial x_1} & \\frac{\\partial \\phi_1}{\\partial x_2} \\\\\n",
    "\\frac{\\partial \\phi_2}{\\partial x_1} & \\frac{\\partial \\phi_2}{\\partial x_2} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "\\frac{\\partial \\phi_{n^2}}{\\partial x_1} & \\frac{\\partial \\phi_{n^2}}{\\partial x_2}\n",
    "\\end{bmatrix}$ $=$ $ \\begin{bmatrix}\n",
    "\\frac{2}{\\sigma^2} (p_{1,1} - x_1) e^{-\\frac{\\|p_{1} - x\\|^2}{\\sigma^2}} & \\frac{2}{\\sigma^2} (p_{1,2} - x_2) e^{-\\frac{\\|p_{1} - x\\|^2}{\\sigma^2}} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "\\frac{2}{\\sigma^2} (p_{n^2,1} - x_1) e^{-\\frac{\\|p_{n^2} - x\\|^2}{\\sigma^2}} & \\frac{2}{\\sigma^2} (p_{n^2,2} - x_2) e^{-\\frac{\\|p_{n^2} - x\\|^2}{\\sigma^2}}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$D\\phi(x)[u] =$ $ \\begin{bmatrix}\n",
    "\\frac{2}{\\sigma^2} (p_{1,1} - x_1) e^{-\\frac{\\|p_{1} - x\\|^2}{\\sigma^2}} & \\frac{2}{\\sigma^2} (p_{1,2} - x_2) e^{-\\frac{\\|p_{1} - x\\|^2}{\\sigma^2}} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "\\frac{2}{\\sigma^2} (p_{n^2,1} - x_1) e^{-\\frac{\\|p_{n^2} - x\\|^2}{\\sigma^2}} & \\frac{2}{\\sigma^2} (p_{n^2,2} - x_2) e^{-\\frac{\\|p_{n^2} - x\\|^2}{\\sigma^2}}\n",
    "\\end{bmatrix}$ $\\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix}$\n",
    "\n",
    "\n",
    "$D\\phi(x)[u] = \\begin{bmatrix} \\frac{2}{\\sigma^2} \\left[ (p_{1,1} - x_1) u_1 + (p_{1,2} - x_2) u_2 \\right] e^{-\\frac{\\|p_1 - x\\|^2}{\\sigma^2}} \\\\ \\frac{2}{\\sigma^2} \\left[ (p_{2,1} - x_1) u_1 + (p_{2,2} - x_2) u_2 \\right] e^{-\\frac{\\|p_2 - x\\|^2}{\\sigma^2}} \\\\ \\vdots \\\\ \\frac{2}{\\sigma^2} \\left[ (p_{n^2,1} - x_1) u_1 + (p_{n^2,2} - x_2) u_2 \\right] e^{-\\frac{\\|p_{n^2} - x\\|^2}{\\sigma^2}} \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "The adjoint of $D\\phi(x): \\mathbb{R}^2 \\to \\mathbb{R}^{n^2}$ is $ A^{\\star}$ such that: \n",
    "\n",
    "$\n",
    "\\langle v, D\\phi(x)[u] \\rangle = \\langle A^{\\star} v, u \\rangle \\quad \\forall v,u \\in \\mathbb{R}^2\n",
    "$\n",
    "\n",
    "which is equivalent to:\n",
    "\n",
    "$\n",
    "v^T D\\phi(x) u = v^T (A^{\\star})^T u \\quad \\forall v,u\\in \\mathbb{R}^2\n",
    "$\n",
    "\n",
    "equivalent to:\n",
    "\n",
    "$\n",
    "A^{\\star} = D\\phi(x)^T\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "\n",
    "Define  $g_1 : \\mathbb{R}^{n^2} \\rightarrow \\mathbb{R}$ by\n",
    "$\n",
    "g_1(x) = \\| x \\|^2\n",
    "$\n",
    "\n",
    "The Jacobian of $ g_1 $ is $ Dg_1(x) = 2x^T $.\n",
    "\n",
    "Then $ f(X) = \\frac{1}{2n^2} \\| \\Phi(X) - y \\|^2 $ $= \\frac{1}{2n^2} g_1(\\Phi(X) - y)$ \n",
    "\n",
    "\n",
    "$\n",
    "Df(X) = \\frac{1}{2n^2} Dg_1 \\circ (\\Phi(X) - y)  = \\frac{1}{2n^2} \\left[ Dg_1(\\Phi(X) - y) D(\\Phi(X-y) \\right]\n",
    "$\n",
    "$\n",
    "= \\frac{1}{n^2} (\\Phi(X) - y)^T D\\Phi(X)\n",
    "$\n",
    "\n",
    "And using the chain rule we have:\n",
    "\n",
    "$$ Df(x)[v] = \\frac{1}{n^2}< \\Phi(x) - y,D \\Phi(x)[v] > = \\frac{1}{n^2}<(D\\Phi(x))^*[\\Phi(x) - y],v> $$\n",
    "\n",
    "$$ \\nabla f(x) = \\frac{(D\\Phi(x))^*[\\Phi(x) - y]}{n^2} $$\n",
    "\n",
    "\n",
    "To be able to keep the Jacobians as square matrices and not 3-dim tensors, we flatten X:\n",
    "\n",
    "Going from $ X = \\begin{bmatrix} X_{11} & \\dots & X_{1K} \\\\ X_{21} & \\dots & X_{2K} \\end{bmatrix} \\in \\mathbb{R}^{2 \\times K} $\n",
    "\n",
    "to $X = \\begin{bmatrix} X_{11} & X_{21} & X_{12} & X_{22} & \\dots & X_{1K} & X_{2K} \\end{bmatrix} \\in \\mathbb{R}^{2K} $.\n",
    "\n",
    "And so we have:\n",
    "\n",
    "$\n",
    "D\\Phi(X) = \\frac{\\partial \\Phi(X)}{\\partial X} = \\begin{bmatrix} \\frac{\\partial}{\\partial X_{11}} \\sum_{k=1}^{K}\\varphi(X_{11}, X_{21}) \\; \\frac{\\partial}{\\partial X_{21}} \\sum_{k=1}^{K}\\varphi(X_{11}, X_{21}) & \\dots &  \\frac{\\partial}{\\partial X_{1K}} \\sum_{k=1}^{K}\\varphi(X_{1K}, X_{2K}) \\; \\frac{\\partial}{\\partial X_{2K}} \\sum_{k=1}^{K}\\varphi(X_{1K}, X_{2K}) \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "= \\begin{bmatrix} \\frac{\\partial}{\\partial X_{11}} \\varphi(X_{11}, X_{21}) \\frac{\\partial}{\\partial X_{21}} \\varphi(X_{11}, X_{21}) & \\dots & \\frac{\\partial}{\\partial X_{1K}} \\varphi(X_{1K}, X_{2K}) \\frac{\\partial}{\\partial X_{2K}} \\varphi(X_{1K}, X_{2K}) \\end{bmatrix} \n",
    "$\n",
    "\n",
    "$\n",
    "= \\begin{bmatrix} D\\varphi(X_{11}, X_{21}) & \\dots & D\\varphi(X_{1K}, X_{2K}) \\end{bmatrix}   \\in \\mathbb{R}^{n^2 \\times 2K}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_phi_i_0(x,p_i):\n",
    "    \"\"\"\n",
    "    Computes value of cell i_0 of jacobian of phi\n",
    "    \n",
    "    :x: np.ndarray[(2, 1)]\n",
    "    :p_i: np.ndarray[(2, 1)]\n",
    "    :returns: float\n",
    "    \"\"\"\n",
    "    return (2/sigmasquared)*(p_i[0]-x[0])*np.e**((-1/sigmasquared)*np.inner(p_i-x,p_i-x))\n",
    "\n",
    "def d_phi_i_1(x,p_i):\n",
    "    \"\"\"\n",
    "    Computes value of cell i_1 of jacobian of phi\n",
    "    \n",
    "    :x: np.ndarray[(2, 1)]\n",
    "    :p_i: np.ndarray[(2, 1)]\n",
    "    :returns: float\n",
    "    \"\"\"\n",
    "    return (2/sigmasquared)*(p_i[1]-x[1])*np.e**((-1/sigmasquared)*np.inner(p_i-x,p_i-x))\n",
    "    \n",
    "def d_phi(x, P):\n",
    "    \"\"\"\n",
    "    Computes the jacobian of small phi\n",
    "\n",
    "    :x: np.ndarray[(1, 2)]\n",
    "    :P: np.ndarray[(2, n**2)]\n",
    "    :returns: np.ndarray[(n**2, 2)]\n",
    "    \"\"\"\n",
    "    # Subtract x from each column of P\n",
    "    diff = P.T - x\n",
    "    # Compute the squared differences\n",
    "    squared_diff = np.sum(diff**2, axis=1)\n",
    "    # Compute the exponential term for all elements at once\n",
    "    exp_term = np.exp((-1 / sigmasquared) * squared_diff)\n",
    "    # Compute the values for d_phi_i_0 and d_phi_i_1 using vectorized operations\n",
    "    \"\"\"can we just do the multiplication of the matrix directly here?\"\"\"\n",
    "    d_phi_0 = (2 / sigmasquared) * diff[:, 0] * exp_term\n",
    "    d_phi_1 = (2 / sigmasquared) * diff[:, 1] * exp_term\n",
    "    # Stack the results to form the final array\n",
    "    d_phi = np.stack((d_phi_0, d_phi_1), axis=1)\n",
    "    return d_phi\n",
    "\n",
    "def d_big_phi(X,P):\n",
    "    \"\"\"\n",
    "    Computes the jacobian of big phi\n",
    "\n",
    "    :X: np.ndarray[(1, 2K)]\n",
    "    :P: np.ndarray[(2, n**2)]\n",
    "    :returns: np.ndarray[(n**2, 2K)]\n",
    "    \"\"\"\n",
    "    d_big_phi = np.zeros((n**2,2*K))\n",
    "    for i in range(K):\n",
    "        d_big_phi[0:n**2,2*i:2*i+2] = d_phi(X[2*i:2*i+2].T,P)\n",
    "    return d_big_phi\n",
    "\n",
    "#compute gradient of f\n",
    "def grad_f(X,P):\n",
    "    \"\"\"\n",
    "    Computes the gradient of f\n",
    "\n",
    "    :X: np.ndarray[(1, 2K)]\n",
    "    :P: np.ndarray[(2, n**2)]\n",
    "    :returns: np.ndarray[(2K, 1)]\n",
    "    \"\"\"\n",
    "    return ((1/n**2)*(bigphi(X,P)-y).T@d_big_phi(X,P)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialising variables from data (set)\n",
    "K = int(data['K'])\n",
    "P = data['P']\n",
    "X0 = data['X0']\n",
    "d = int(data['d'])\n",
    "delta_0 = data['delta_0']\n",
    "delta_bar = data['delta_bar']\n",
    "n = int(data['n'])\n",
    "sigma = float(data['sigma'])\n",
    "y = data['y'].flatten(order='F').reshape((n**2,1), order=\"F\")\n",
    "\n",
    "#check that the gradient is correct using taylor expansion\n",
    "# f(x+tv)= f(x) + t<v,grad_f(x)> + O(t^2)\n",
    "\n",
    "# Generate a random point and a random direction\n",
    "theta = np.random.uniform(-0.5, 0.5, (d, K)).flatten(order='F').reshape((d*K,1), order=\"F\")\n",
    "v = np.random.uniform(-0.5, 0.5, (d, K)).flatten(order='F').reshape((d*K,1), order=\"F\")\n",
    "v = v / np.linalg.norm(v)\n",
    "\n",
    "## Check the gradient \n",
    "def checkgradient(f,grad_f, theta,v):\n",
    "    #logspace of t values\n",
    "    t=np.logspace(-8, 0, num=100)\n",
    "    #intialise error to 0\n",
    "    error = np.zeros_like(t)\n",
    "    #pre-calculae f_lambda and f_lambda_grad to use in for loop\n",
    "    f_lambda = f(theta,P,y)\n",
    "    f_lambda_grad = grad_f(theta,P)\n",
    "    #compute the error at each t\n",
    "    for i in tqdm(range(100)):\n",
    "        error[i] = np.abs( f(theta+(t[i]*v),P,y)-f_lambda-(t[i]*v.T@f_lambda_grad) )\n",
    "        \n",
    "    #plot the graph of error vs t\n",
    "    plt.loglog(t,error)\n",
    "    plt.xlabel('t (log scale)')\n",
    "    plt.ylabel('Error (log scale)')\n",
    "    plt.title('Plot of t vs Error')\n",
    "    plt.grid()\n",
    "    plt.show\n",
    "checkgradient(f,grad_f,theta,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "\n",
    "## Computing the Hessian \n",
    "\n",
    "First we re-derive the gradient of f using the chain rule:\n",
    "\n",
    "$$ f(x) = \\frac{1}{2n^2}||\\Phi(x) - y ||^2$$\n",
    "\n",
    "$$ Df(x)[v] = \\frac{1}{n^2}< \\Phi(x) - y,D \\Phi(x)[v] > = \\frac{1}{n^2}<(D\\Phi(x))^*[\\Phi(x) - y],v> $$\n",
    "\n",
    "$$ \\nabla f(x) = \\frac{(D\\Phi(x))^*[\\Phi(x) - y]}{n^2} $$\n",
    "\n",
    "Then, using the above calculations, we can calculate the hessian using the product rule:\n",
    "\n",
    "$$ \\nabla^2 f(x)[v] = \\frac{1}{n^2}[(D(D\\Phi(x))[v]^*)[\\Phi(x)-y]+D\\Phi(x)^*[D\\Phi(x)[v]]] $$\n",
    "\n",
    "\n",
    "---\n",
    "For the calculation of $D(D\\Phi(X))[v]$\n",
    "\n",
    "$\n",
    "\\Phi : \\mathbb{R}^{2K} \\rightarrow \\mathbb{R}^{n^2}\n",
    "$\n",
    "\n",
    "$\n",
    "J := D\\Phi : \\mathbb{R}^{2K} \\rightarrow \\mathbb{R}^{n^2}\n",
    "$\n",
    "\n",
    "$\n",
    "\\varphi : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^{n^2}\n",
    "$\n",
    "$\n",
    "D\\varphi : \\mathbb{R} \\rightarrow \\mathbb{R}^{n^2 \\times 2}\n",
    "$\n",
    "\n",
    "We define:\n",
    "\n",
    "$\n",
    "g_1(\\vec{x})(\\vec{p}) = \\frac{2}{\\sigma^2} (\\vec{p}, x_1) e^{-\\frac{1}{\\sigma^2} \\| \\vec{p} - x_1 \\|^2}\n",
    "$\n",
    "\n",
    "$\n",
    "g_2(\\vec{x})(\\vec{p}) = \\frac{2}{\\sigma^2} (\\vec{p}, x_2) e^{-\\frac{1}{\\sigma^2} \\| \\vec{p} - x_2 \\|^2}\n",
    "$\n",
    "\n",
    "Their jacobians being: \n",
    "\n",
    "$\n",
    "Dg_1(\\vec{x})(\\vec{p}) = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} g_1(\\vec{x})(\\vec{p}) & \\frac{\\partial}{\\partial x_2} g_1(\\vec{x})(\\vec{p}) \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "= \\begin{bmatrix} \\frac{4}{\\sigma^4} (p_1-x_1)^2  e^{-\\frac{1}{\\sigma^2} \\| p - x \\|^2} - \\frac{2}{\\sigma^2}e^{-\\frac{1}{\\sigma^2} \\| p - x \\|^2} \\;\\; \\frac{4}{\\sigma^4} (p_1-x_1)(p_2-x_2)  e^{-\\frac{1}{\\sigma^2} \\| p - x \\|^2}  \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "Dg_2(\\vec{x})(\\vec{p}) = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} g_2(\\vec{x})(\\vec{p}) & \\frac{\\partial}{\\partial x_2} g_2(\\vec{x})(\\vec{p}) \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "= \\begin{bmatrix}  \\frac{4}{\\sigma^4} (p_1-x_1)(p_2-x_2)  e^{-\\frac{1}{\\sigma^2} \\| p - x \\|^2} \\;\\; \\frac{4}{\\sigma^4} (p_2-x_2)^2  e^{-\\frac{1}{\\sigma^2} \\| p - x \\|^2} - \\frac{2}{\\sigma^2}e^{-\\frac{1}{\\sigma^2} \\| p - x \\|^2}  \\end{bmatrix}\n",
    "$\n",
    "\n",
    "---\n",
    "$\n",
    "D(D\\Phi(X))[v] = DJ(X)[v]\n",
    "$\n",
    "\n",
    "$\n",
    "= \\lim_{t \\to 0} \\frac{J(X + tv) - J(X)}{t}\n",
    "$\n",
    "\n",
    "$\n",
    "= \\lim_{t \\to 0} \\frac{D\\Phi(X + t V) - D\\Phi(X)}{t}\n",
    "$\n",
    "\n",
    "$\n",
    "= \\begin{bmatrix} \\lim_{t \\to 0} \\frac{D\\varphi(x_1 + tv_1) - D\\varphi(x_1)}{t} \\;\\; \\ldots \\;\\; \\lim_{t \\to 0} \\frac{D\\varphi(x_K + tv_K) - D\\varphi(x_K)}{t} \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "= \\begin{bmatrix}\n",
    "    \\begin{bmatrix} Dg_1(x_1)(p_1)[v_1] & Dg_2(x_1)(p_1)[v_1] \\\\ \\vdots & \\vdots \\\\ Dg_1(x_1)(p_n)[v_1] & Dg_2(x_1)(p_n)[v_1] \\end{bmatrix} &\n",
    "    \\cdots &\n",
    "    \\begin{bmatrix} Dg_1(x_K)(p_1)[v_K] & Dg_2(x_K)(p_1)[v_K] \\\\ \\vdots & \\vdots \\\\ Dg_1(x_K)(p_n)[v_K] & Dg_2(x_K)(p_n)[v_K] \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_g(x,P,v):\n",
    "    #subtract x from each column of P\n",
    "    diff = P.T-x\n",
    "    # Compute the squared differences\n",
    "    squared_diff = diff**2\n",
    "    norm_squared = np.sum(diff**2, axis=1)\n",
    "    # Compute the exponential term for all elements at once\n",
    "    a_1 = np.exp((-1 / sigmasquared) * norm_squared)*(2 / sigmasquared)**2\n",
    "    # Compute the values for d_phi_i_0 and d_phi_i_1 using vectorized operations\n",
    "    d_g_1_0 = a_1 *(v[0]*(squared_diff[:,0]-(sigmasquared/2))+v[1]*diff[:,0]*diff[:,1])\n",
    "    d_g_1_1 = a_1 *(v[1]*(squared_diff[:,1]-(sigmasquared/2))+v[0]*diff[:,0]*diff[:,1])\n",
    "    # Stack the results to form the final array\n",
    "    d_g = np.stack((d_g_1_0, d_g_1_1), axis=1)\n",
    "    return d_g\n",
    "    \n",
    "\n",
    "\n",
    "def d_d_phi(X, P, V):\n",
    "    \"\"\"\n",
    "    Compute the directional derivative of the Jacobian of phi at x in the direction v.\n",
    "\n",
    "    :param X: np.ndarray[(1, 2K)] \"position of a stat\"\n",
    "    :param P: np.ndarray[(2, n**2)] \"pixel positions\"\n",
    "    :param V: np.ndarray[(1, 2K)] \"vector direction of change in star position\"\n",
    "    :returns: np.ndarray[(n**2, 2K)]\n",
    "    \"\"\"\n",
    "    d_d_phi = np.zeros((n**2,2*K))\n",
    "    for i in range(K):\n",
    "            d_d_phi[0:n**2,2*i:2*i+2] = d_g(X[2*i:2*i+2].T,P,V[2*i:2*i+2])\n",
    "    return d_d_phi\n",
    "    \n",
    "\n",
    "\n",
    "def hessian_f(X,V):\n",
    "    \"\"\"\n",
    "    Compute the Hessian of f at X in the direction v.\n",
    "    \n",
    "    :param X: np.ndarray[(1, 2K)] \"position of K stars\"\n",
    "    :param P: np.ndarray[(2, n**2)] \"pixel positions\"\n",
    "    :param y: np.ndarray[(n**2, 1)] \"actual image detected, y = Phi(X_true)\" remember we want to find X_true\n",
    "    :param v: np.ndarray[(2, K)] \"direction at which hessian is taken\" \n",
    "    :returns: np.ndarray[(2*K, 1)]\n",
    "    \"\"\"\n",
    "    # Compute intermediate terms\n",
    "    phi_X = bigphi(X, P) - y  # (n**2, 1)\n",
    "    d_big_phi_X = d_big_phi(X, P)  # (n**2, 2*K)\n",
    "    d_big_phi_X_v = d_big_phi_X @ V  # directional derivative of big_phi in direction v\n",
    "\n",
    "    # Compute each term in the Hessian formula\n",
    "\n",
    "    term1 = d_d_phi(X, P, V).T @ phi_X \n",
    "    term2 = d_big_phi_X.T @ d_big_phi_X_v \n",
    "\n",
    "    hessian = (1 / n**2) * (term1 + term2)\n",
    "    return hessian\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "From $ t = 10^{-8} $ to $ 10^{-5} $, the line deviates from being straight, likely due to round-off errors caused by the small values of $ t $ and the error. This is not a concern, as the trust-region method can handle inaccuracies in the Hessian approximation near the point of interest by expanding the trust region or radius as needed.\n",
    "\n",
    "Beyond this range, the graph forms a straight line with a slope of 3, confirming that the Hessian calculation is correct. \n",
    "\n",
    "We also observe a slight bump around $ t = 10^{-1} $, which can vary depending on the randomized values of $ \\theta $ and $ v $. This is likely due to $ f $ being very small at this point, where the function approaches 0. However, since the values are small, this deviation is not problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random point and a random direction\n",
    "theta = np.random.uniform(-0.5, 0.5, (d, K)).flatten(order='F').reshape((d*K,1), order=\"F\")\n",
    "v = np.random.uniform(-0.5, 0.5, (d, K)).flatten(order='F').reshape((d*K,1), order=\"F\")\n",
    "v = v / np.linalg.norm(v)\n",
    "\n",
    "## Check the hessian \n",
    "def checkhessian(f,grad_f,hessian_f,theta,v):\n",
    "    #logspace of t values\n",
    "    t=np.logspace(-8, 0, num=100)\n",
    "    #intialise error to 0\n",
    "    error = np.zeros_like(t)\n",
    "    #pre-calculae f_lambda and f_lambda_grad to use in for loop\n",
    "    f_lambda = f(theta,P,y)\n",
    "    f_lambda_grad = grad_f(theta,P)\n",
    "    f_lambda_hess_v = hessian_f(theta,v)\n",
    "    #compute the error at each t\n",
    "    for i in tqdm(range(100)):\n",
    "        error[i] = np.abs(f(theta+(t[i]*v),P,y)-f_lambda-(t[i]*v.T@f_lambda_grad)- ((t[i]**2)/2)*v.T@f_lambda_hess_v)\n",
    "        \n",
    "    #plot the graph of error vs t\n",
    "    plt.loglog(t,error)\n",
    "    plt.xlabel('t (log scale)')\n",
    "    plt.ylabel('Error (log scale)')\n",
    "    plt.title('Plot of t vs Error')\n",
    "    plt.grid()\n",
    "    plt.show\n",
    "checkhessian(f,grad_f,hessian_f,theta,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_conjugate_gradient(b: np.array, radius: float, x0) -> tuple:\n",
    "    \"\"\"\n",
    "    Solve the trust-region subproblem using the Truncated Conjugate Gradient (tCG) method.\n",
    "\n",
    "    :param b: np.array[(2, K)] - The gradient vector reshaped as a 2D array\n",
    "    :param radius: float - The trust-region radius for constraint\n",
    "    :param x0: Initial guess or parameter for Hessian computation\n",
    "    :returns: tuple (solution vector, residual vector, bool flag) where:\n",
    "              - The solution vector has shape (2*K, 1),\n",
    "              - The residual vector indicates the end of iterations, and\n",
    "              - A boolean flag shows whether the radius constraint was active.\n",
    "    \"\"\"\n",
    "\n",
    "    # Flatten b to make it a column vector of shape (2*K, 1) for matrix operations.\n",
    "    b = b.reshape((-1, 1), order=\"F\")\n",
    "    v0 = np.zeros((2 * K, 1))  # Initial solution vector\n",
    "    r0 = b.copy()              # Residual vector initialized to b\n",
    "    p0 = b.copy()              # Search direction initialized to b\n",
    "\n",
    "    for i in range(5):\n",
    "        Hp = hessian_f(x0, p0)\n",
    "        \n",
    "        # Calculate alpha (step size) as the ratio of r0.T @ r0 and p0.T @ Hp\n",
    "        inner_pHp = (p0.T @ Hp)[0, 0]  # Inner product of p0 and Hp as scalar\n",
    "        alpha = (r0.T @ r0)[0, 0] / inner_pHp\n",
    "\n",
    "        # Compute potential next solution vector v_plus\n",
    "        v_plus = v0 + alpha * p0\n",
    "        \n",
    "        # Check trust-region radius constraint and whether the problem is solved\n",
    "        if inner_pHp <= 0 or np.linalg.norm(v_plus) >= radius:\n",
    "            # If radius constraint is active, calculate step size `t` for v0 to satisfy ||v0|| = radius\n",
    "            inner_pv = (v0.T @ p0)[0, 0]\n",
    "            norm2_p = (p0.T @ p0)[0, 0]\n",
    "            discriminant = inner_pv**2 - ((v0.T @ v0)[0, 0] - radius**2) * norm2_p\n",
    "            t = (-inner_pv + np.sqrt(discriminant)) / norm2_p\n",
    "            \n",
    "            # Adjust v0 to be on the boundary and return solution\n",
    "            v0 += t * p0\n",
    "            return (v0, b - r0 + t * Hp, True)  # Radius constraint was active, flag is True\n",
    "        \n",
    "        # Otherwise, update the solution vector\n",
    "        v0 = v_plus\n",
    "        r_old = r0.copy()  # Save current residual to compute beta\n",
    "        r0 -= alpha * Hp   # Update residual\n",
    "\n",
    "        # Check for convergence based on residual norm\n",
    "        if np.linalg.norm(r0) <= np.linalg.norm(b) * min(np.linalg.norm(r0), 0.1):\n",
    "            return (v0, b - r0, False)  # Converged within trust-region, flag is False\n",
    "        \n",
    "        # Update beta for next conjugate direction\n",
    "        beta0 = (r0.T @ r0)[0, 0] / (r_old.T @ r_old)[0, 0]\n",
    "        \n",
    "        # Update search direction p0\n",
    "        p0 = r0 + beta0 * p0\n",
    "    \n",
    "    return (v0, b - r0, False)  # Return solution and residual; flag is False as radius was not active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10\n",
    "\n",
    "We use two stopping criteria for the trust-region method. First, if the gradient norm becomes extremely small, we stop, as further steps would have a negligible impact. Second, we limit the number of iterations for the truncated conjugate gradient (TCG) method to 5. This ensures we don't spend excessive computational resources solving the subproblem and instead obtain a rough approximation of the minimizer within the radius.\n",
    "\n",
    "Additionally, we set a maximum number of iterations for the trust-region method itself. This prevents the algorithm from running indefinitely in cases where the function has many local minima, which could cause the method to get stuck without achieving a very low gradient norm. While we could have used a maximum runtime as an alternative criterion, this wasn't necessary. The subproblem involves only a small number of iterations, and the main computational effort calculating the gradient and Hessian is relatively inexpensive. Therefore, using gradient norm and iteration limits as stopping criteria is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trust_region_method(x0, max_radius, delta_0, rho_prime=0.1, max_iters=35):\n",
    "    \"\"\"\n",
    "    Trust Region Method using a quadratic model and truncated conjugate gradient.\n",
    "\n",
    "    :param x0: Initial point for the optimization (numpy array).\n",
    "    :param max_radius: Maximum allowable radius for the trust region.\n",
    "    :param delta_0: Initial radius for the trust region.\n",
    "    :param rho_prime: Threshold for accepting a step based on actual-to-model reduction ratio.\n",
    "    :param max_iters: Maximum number of iterations for the trust region algorithm.\n",
    "    \n",
    "    :functions called: gradient_func: Function to compute the gradient at a point `x`.\n",
    "    :functions called: hessian_func: Function to compute the Hessian matrix at a point `x`.\n",
    "\n",
    "    :returns: The optimized point `x` and a list of gradient norms for each iteration.\n",
    "    \"\"\"\n",
    "    x = x0.copy().flatten(order='F').reshape(d*K,1)\n",
    "    delta = delta_0\n",
    "    gradient_norms = []\n",
    "    initial_time = time.time()\n",
    "    time_array = [] \n",
    "    for k in tqdm(range(max_iters)):\n",
    "        grad = grad_f(x,P)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        gradient_norms.append(grad_norm)  # Store the gradient norm\n",
    "        time_array.append(time.time()-initial_time)\n",
    "\n",
    "        if grad_norm <= 1e-20:\n",
    "            print(\"Gradient close to zero, stopping optimization.\")\n",
    "            break\n",
    "        \n",
    "        # Inner truncated CG with delta as the maximum allowable step\n",
    "        u_k,Hu_k,flag = truncated_conjugate_gradient(-grad, delta,x)\n",
    "        x_plus = x + u_k\n",
    "        \n",
    "        # Calculate the actual and model reductions\n",
    "        actual_reduction = f(x,P,y) - f(x_plus,P,y)\n",
    "        model_reduction = -grad.T @ u_k - 0.5 * u_k.T @ Hu_k\n",
    "        \n",
    "        # Compute the ratio of actual to model improvement\n",
    "        rho_k = actual_reduction / (model_reduction)  # Avoid division by zero\n",
    "\n",
    "        # Accept or reject the tentative next iterate\n",
    "        if rho_k > rho_prime:\n",
    "            x = x_plus  # Accept the step\n",
    "        else:\n",
    "            x = x  # Reject the step \n",
    "\n",
    "        # Update the trust-region radius based on rho_k\n",
    "        if rho_k < 0.25:\n",
    "            delta = 0.25 * delta\n",
    "        elif rho_k > 0.75 and flag:\n",
    "            delta = min(2 * delta, max_radius)\n",
    "        else:\n",
    "            delta = delta\n",
    "    return x.reshape((2, K), order=\"F\"), gradient_norms, time_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 11\n",
    "Run the trust-region algorithm with truncated-CG on data-toy.mat starting from the initial point x0 provided in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Archived code for plotting the stars on R^2 as discrete points.\n",
    "\"\"\"\n",
    "# def plotting_stars(Random_intial_point:np.array = X0, Delta0: float = delta_0, DeltaBar: float = np.sqrt(6),  rhoPrime: float=0.1):\n",
    "#     \"\"\"\n",
    "#     usual input: random initial starting point for Trsolver, usually taken to be given by the dataset\n",
    "\n",
    "#     :param Delta0: float\n",
    "#     :param DeltaBar: float\n",
    "#     :param rhoPrime: float\n",
    "#     :param tol: float\n",
    "#     param time_limit: float\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # x0 = X0.flatten(order='F').reshape(d*K,1) #x0 \n",
    "#     points = trust_region_method(Random_intial_point, DeltaBar, Delta0, rhoPrime, max_iters=30)[0]\n",
    "\n",
    "#     # Separate x and y coordinates for plotting\n",
    "#     x = points[0,:]\n",
    "#     y = points[1,:]\n",
    "\n",
    "#     # Create the scatter plot\n",
    "#     plt.scatter(x, y, color='blue', marker='o')\n",
    "#     plt.xlabel('x')\n",
    "#     plt.ylabel('y')\n",
    "#     plt.title('Stars plotted on R^2')\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plotimage(X):\n",
    "    \"\"\"\n",
    "    Plots the image of matrix X with a colorbar.\n",
    "    \n",
    "    Parameters:\n",
    "    X : 2D array-like\n",
    "        Input matrix to be plotted.\n",
    "    \"\"\"\n",
    "    X = np.array(X)\n",
    "    m, n = X.shape\n",
    "    if m != n:\n",
    "        print(\"Warning: plotimage - Input matrix is not square.\")\n",
    "    \n",
    "    x = np.linspace(-0.5, 0.5, m)\n",
    "    y = np.linspace(-0.5, 0.5, n)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(X, extent=[x[0], x[-1], y[0], y[-1]], aspect='equal', origin='lower')\n",
    "    plt.colorbar()\n",
    "    plt.axis('tight')\n",
    "    plt.draw()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#test with X0 given for data toy\n",
    "# re-initialising variables to toy data (set)\n",
    "K = int(data_toy['K'])\n",
    "P = data_toy['P']\n",
    "X0 = data_toy['X0']\n",
    "d = int(data_toy['d'])\n",
    "delta_0 = data_toy['delta_0']\n",
    "delta_bar = data_toy['delta_bar']\n",
    "n = int(data_toy['n'])\n",
    "sigma = float(data_toy['sigma'])\n",
    "y = data_toy['y'].flatten(order='F').reshape((n**2,1), order=\"F\")\n",
    "\n",
    "solution,grad_norms5,times = trust_region_method(data_toy['X0'],delta_bar, delta_0)\n",
    "# print(\"Positions we found\")\n",
    "# plotting_stars(data_toy['X0'])\n",
    "print(\"What was observed from positions we found\")\n",
    "plotimage(bigphi(solution.reshape((2, K), order=\"F\"),P).reshape((n,n), order=\"F\"))\n",
    "print(\"What was observed from real positions\")\n",
    "plotimage(y.reshape((n,n), order=\"F\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 12\n",
    "Run the trust-region method several times on data.mat with different random initial points.\n",
    "\n",
    "Since the function is non-convex, the trust-region algorithm might converge to different local minima depending on the initial point, leading to varying objective values across runs. If the stars in the dataset are well-separated (like in toy data sets), the results might be more consistent as the algorithm converges near true positions. However, for data.mat, where stars are closer and configurations more complex, we're more likely to see variability in the objective values due to multiple plausible configurations. \n",
    "\n",
    "The variability in the objective values across runs can be explained by the presence of multiple local minima in the function. These local minima are closely tied to the number of stars in the dataset. Specifically, the function 𝑓 tends to decrease significantly at configurations corresponding to plausible star positions. This behavior results in the function dropping near these points, creating multiple potential minima where the trust-region algorithm can converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialising variables to data (set)\n",
    "K = int(data['K'])\n",
    "P = data['P']\n",
    "X0 = data['X0']\n",
    "d = int(data['d'])\n",
    "delta_0 = data['delta_0']\n",
    "delta_bar = data['delta_bar']\n",
    "n = int(data['n'])\n",
    "sigma = float(data['sigma'])\n",
    "y = data['y'].flatten(order='F').reshape((n**2,1), order=\"F\")\n",
    "\n",
    "#test with random point 1\n",
    "x_random1 = np.random.uniform(-0.5, 0.5, (d, K))\n",
    "solution,grad_norms5,times = trust_region_method(x_random1,delta_bar, delta_0)\n",
    "print(f\"What was observed from found positions with starting random point {x_random1}\")\n",
    "plotimage(bigphi(solution.reshape((2, K), order=\"F\"),P).reshape((n,n), order=\"F\"))\n",
    "\n",
    "#test with random point 2\n",
    "x_random2 = np.random.uniform(-0.5, 0.5, (d, K))\n",
    "solution,grad_norms5,times = trust_region_method(x_random2,delta_bar, delta_0)\n",
    "print(f\"What was observed from found positions with starting random point {x_random2}\")\n",
    "plotimage(bigphi(solution.reshape((2, K), order=\"F\"),P).reshape((n,n), order=\"F\"))\n",
    "\n",
    "#test with random point 3\n",
    "x_random3 = np.random.uniform(-0.5, 0.5, (d, K))\n",
    "solution,grad_norms5,times = trust_region_method(x_random3,delta_bar, delta_0)\n",
    "print(f\"What was observed from found positions with starting random point {x_random3}\")\n",
    "plotimage(bigphi(solution.reshape((2, K), order=\"F\"),P).reshape((n,n), order=\"F\"))\n",
    "\n",
    "print(\"What was observed from real positions\")\n",
    "plotimage(y.reshape((n,n), order=\"F\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 13\n",
    "Run the trust-region algorithm on data.mat starting from the point x0 provided in thedataset.\n",
    "\n",
    "Plot the norm of the gradient ∥∇f(xk)∥ as a function of k (gradient norm always on a log-scale). Do the same thing as a function of computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting function for gradient norm vs iteration number\n",
    "def plot_gradient_norm_iterations(gradient_norms:np.ndarray):\n",
    "    plt.plot(gradient_norms, marker='o', linestyle='-', color='b')\n",
    "    plt.title(\"Gradient Norm vs. Iteration Number\") # need to maybe change this to a different title ###################\n",
    "    plt.xlabel(\"Iteration Number\")\n",
    "    plt.ylabel(\"Gradient Norm (log scale)\")\n",
    "    plt.yscale('log')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# plotting function for gradient norm vs iteration number\n",
    "def plot_gradient_norm_time(gradient_norms1: list, time_array1: list):\n",
    "    plt.plot(time_array1, gradient_norms1, marker='o', linestyle='-', color='b')\n",
    "    plt.title(\"Gradient Norm vs. Computation Time\")\n",
    "    plt.xlabel(\"Computation Time (seconds)\")\n",
    "    plt.ylabel(\"Gradient Norm (log scale)\")\n",
    "    plt.yscale('log')\n",
    "    plt.grid()\n",
    "    plt.legend() \n",
    "    plt.show()\n",
    "\n",
    "#test with X0 given\n",
    "solution,grad_norms,time_array = trust_region_method(X0,delta_bar, delta_0)\n",
    "plot_gradient_norm_iterations(grad_norms)\n",
    "plot_gradient_norm_time(grad_norms,time_array)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
